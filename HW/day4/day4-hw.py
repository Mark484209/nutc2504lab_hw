import os
import json
import base64
import requests
from typing import List, TypedDict, Literal
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END
from playwright.sync_api import sync_playwright

# --- 1. è¨­å®šå€åŸŸ ---
SEARXNG_URL = "https://puli-8080.huannago.com/search"

# å»ºè­°åŠ ä¸Š max_retries èˆ‡ timeout ä»¥æ‡‰å°ä¹‹å‰é‡åˆ°çš„ 524 è¶…æ™‚å•é¡Œ
llm = ChatOpenAI(
    base_url="https://ws-02.wade0426.me/v1",
    api_key="your_api_key_here", 
    model="google/gemma-3-27b-it",
    temperature=0,
    max_retries=2,
    timeout=120
)

# --- 2. ç‹€æ…‹å®šç¾© ---
class AgentState(TypedDict):
    question: str
    keywords: str
    knowledge_base: str
    cache_hit: bool
    final_answer: str
    count: int 
    feedback: str

# --- 3. æ ¸å¿ƒå·¥å…·å‡½æ•¸ ---
def search_searxng(query: str, limit: int = 2):
    params = {"q": query, "format": "json", "language": "zh-TW"}
    try:
        response = requests.get(SEARXNG_URL, params=params, timeout=10)
        return [r for r in response.json().get('results', []) if 'url' in r][:limit]
    except Exception as e:
        print(f"âŒ æœå°‹å‡ºéŒ¯: {e}")
        return []

def vlm_analyze_page(url: str, question: str):
    print(f"ğŸ“¸ [VLM] å•Ÿå‹•è¦–è¦ºé–±è®€: {url}")
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page(viewport={'width': 1280, 'height': 800})
            page.goto(url, wait_until="domcontentloaded", timeout=15000)
            page.wait_for_timeout(2000)
            img_b64 = base64.b64encode(page.screenshot()).decode('utf-8')
            browser.close()
            
            msg = HumanMessage(content=[
                {"type": "text", "text": f"åˆ†ææ­¤æˆªåœ–å…§å®¹ä¸¦é‡å°å•é¡Œ '{question}' æä¾›é—œéµè³‡è¨Šã€‚"},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_b64}"}}
            ])
            return llm.invoke([msg]).content
    except Exception as e:
        return f"ç¶²é é–±è®€å¤±æ•—: {e}"

# --- 4. LangGraph ç¯€é»å¯¦ä½œ ---

def check_cache(state: AgentState):
    print("\n[Node] 1. æª¢æŸ¥å¿«å–...")
    return {"cache_hit": False, "knowledge_base": "", "count": 0, "feedback": ""}

def query_gen(state: AgentState):
    new_count = state.get("count", 0) + 1
    fb = f"\nå‰æ¬¡æ€è€ƒåé¥‹ï¼š{state['feedback']}" if state['feedback'] else ""
    print(f"ğŸ”„ [Node] 2. ç¬¬ {new_count}/3 æ¬¡æœå°‹ - ç”Ÿæˆé—œéµå­—...")
    
    prompt = f"å•é¡Œï¼š'{state['question']}'{fb}\nè«‹ç”¢å‡ºä¸€å€‹ç²¾æº–çš„æœå°‹é—œéµå­—ï¼ˆåƒ…è¼¸å‡ºå­—ä¸²å…§å®¹ï¼‰ã€‚"
    keyword = llm.invoke(prompt).content.strip().replace('"', '')
    return {"keywords": keyword, "count": new_count}

def search_tool(state: AgentState):
    print(f"ğŸ” [Node] 3. åŸ·è¡Œæª¢ç´¢: {state['keywords']}")
    results = search_searxng(state['keywords'])
    info = ""
    for r in results:
        analysis = vlm_analyze_page(r['url'], state['question'])
        info += f"\n[ä¾†æº: {r['title']}]\n{analysis}\n"
    return {"knowledge_base": state['knowledge_base'] + info}

# â­ æ–°å¢ç¯€é»ï¼šè³‡è¨Šç²¾ç…‰ (Research Refiner)
def research_refiner(state: AgentState):
    print("ğŸ§¹ [Node] 4. è³‡è¨Šç²¾ç…‰ - éæ¿¾é›œè¨Š...")
    if not state['knowledge_base']:
        return {"knowledge_base": "å°šæœªå–å¾—æœ‰æ•ˆè³‡è¨Š"}
    
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹è³‡æ–™è™•ç†å°ˆå®¶ã€‚è«‹æ ¹æ“šå•é¡Œæ•´ç†ç›®å‰çš„æœå°‹è³‡è¨Šã€‚
    å•é¡Œï¼š{state['question']}
    
    åŸå§‹è³‡æ–™ï¼š
    {state['knowledge_base']}
    
    è«‹ç§»é™¤å»£å‘Šã€é‡è¤‡å…§å®¹ï¼Œå°‡äº‹å¯¦ä»¥æ¢åˆ—å¼æ‘˜è¦æ•´ç†ã€‚å¦‚æœè³‡è¨Šè¡çªï¼Œè«‹ä¸¦åˆ—èªªæ˜ã€‚
    """
    refined_info = llm.invoke(prompt).content
    return {"knowledge_base": refined_info}

def planner(state: AgentState):
    print(f"ğŸ§  [Node] 5. Planner è©•ä¼°ä¸­...")
    prompt = f"""
    è©•ä¼°ç¾æœ‰è³‡è¨Šæ˜¯å¦è¶³ä»¥å®Œæ•´å›ç­”å•é¡Œã€‚
    å•é¡Œï¼š{state['question']}
    ç¾æœ‰ç²¾ç…‰è³‡è¨Šï¼š{state['knowledge_base']}
    
    è«‹ä»¥ JSON æ ¼å¼å›å‚³ï¼š
    {{
        "sufficient": "YES" æˆ– "NO",
        "feedback": "è‹¥ç‚º NOï¼Œè«‹èªªæ˜é‚„ç¼ºå°‘ä»€éº¼é—œéµè³‡è¨Šï¼Ÿ"
    }}
    """
    res = llm.invoke(prompt).content
    try:
        data = json.loads(res[res.find("{"):res.rfind("}")+1])
        decision = data.get("sufficient", "NO")
        feedback = data.get("feedback", "è³‡è¨Šä»ä¸è¶³")
    except:
        decision = "NO"
        feedback = "ç„¡æ³•è§£ææ€è€ƒå…§å®¹"

    # å°‡æ±ºç­–æš«å­˜åœ¨ final_answer æ¬„ä½ä¾›è·¯å¾‘åˆ¤æ–·ä½¿ç”¨
    return {"feedback": feedback, "final_answer": decision}

def final_answer(state: AgentState):
    print("ğŸ“¢ [Node] 6. ç”Ÿæˆæœ€çµ‚å ±å‘Š...")
    prompt = f"è«‹æ ¹æ“šä»¥ä¸‹æŸ¥è­‰äº‹å¯¦ï¼Œç‚ºä½¿ç”¨è€…å¯«ä¸€ä»½å°ˆæ¥­ã€å®¢è§€çš„å ±å‘Šï¼š\n{state['knowledge_base']}\nå•é¡Œï¼š{state['question']}"
    res = llm.invoke(prompt).content
    return {"final_answer": res}

# --- 5. æ§‹å»ºæµç¨‹åœ– ---
workflow = StateGraph(AgentState)

workflow.add_node("check_cache", check_cache)
workflow.add_node("query_gen", query_gen)
workflow.add_node("search_tool", search_tool)
workflow.add_node("research_refiner", research_refiner) # <-- åŠ å…¥æ–°ç¯€é»
workflow.add_node("planner", planner)
workflow.add_node("final_answer", final_answer)

workflow.set_entry_point("check_cache")

# è¨­å®šè·¯å¾‘é‚è¼¯
workflow.add_conditional_edges(
    "check_cache",
    lambda x: "final_answer" if x["cache_hit"] else "query_gen",
    {"final_answer": "final_answer", "query_gen": "query_gen"}
)

workflow.add_edge("query_gen", "search_tool")
workflow.add_edge("search_tool", "research_refiner") # æª¢ç´¢å®Œå…ˆç²¾ç…‰
workflow.add_edge("research_refiner", "planner")    # ç²¾ç…‰å¾Œæ‰çµ¦ Planner è©•ä¼°

def route_logic(state: AgentState):
    # å¦‚æœ Planner èªªå¤ äº† (YES) æˆ–è€…æ¬¡æ•¸åˆ°äº† (>=3) å°±çµæ¡ˆ
    if state.get("count", 0) >= 3 or "YES" in state.get("final_answer", ""):
        return "final_answer"
    return "query_gen"

workflow.add_conditional_edges(
    "planner",
    route_logic,
    {"final_answer": "final_answer", "query_gen": "query_gen"}
)

workflow.add_edge("final_answer", END)
app = workflow.compile()

# --- 6. è¼¸å‡ºæµç¨‹åœ–èˆ‡åŸ·è¡Œ ---
print("\n" + "="*20 + " ç³»çµ±æ¶æ§‹åœ– " + "="*20)
# åœ¨ç¨‹å¼åŸ·è¡Œå‰å…ˆåˆ—å° ASCII æµç¨‹åœ–
app.get_graph().print_ascii()
print("="*55 + "\n")



if __name__ == "__main__":
    q = input("è«‹è¼¸å…¥æŸ¥è­‰å•é¡Œï¼š")
    # é–‹å§‹ä¸²æµåŸ·è¡Œ
    for output in app.stream({"question": q, "knowledge_base": "", "cache_hit": False, "count": 0}):
        for node, data in output.items():
            # ç•¶é‹è¡Œåˆ° final_answer ç¯€é»å®Œæˆæ™‚ï¼Œè¼¸å‡ºçµæœ
            if node == "final_answer" and "final_answer" in data:
                # ç¢ºä¿æˆ‘å€‘æ‹¿åˆ°çš„æ˜¯æœ€çµ‚å ±å‘Šå­—ä¸²ï¼Œè€Œé Planner çš„ YES/NO
                if len(data["final_answer"]) > 5: 
                    print("\n" + "âœ¨"*10 + " æŸ¥è­‰å ±å‘Š " + "âœ¨"*10)
                    print(data["final_answer"])
                    
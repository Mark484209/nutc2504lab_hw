import os
import re
import base64
import requests
import json
from typing import Annotated, TypedDict
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END, add_messages
from playwright.sync_api import sync_playwright

# --- 1. åˆå§‹åŒ– LLM (è¨­å®šè¶…æ™‚èˆ‡é‡è©¦é˜²æ­¢ SSL å´©æ½°) ---
llm = ChatOpenAI(
    base_url="https://ws-02.wade0426.me/v1",
    api_key="", 
    model="google/gemma-3-27b-it",
    temperature=0,
    timeout=45,
    max_retries=2
)

SEARXNG_URL = "https://puli-8080.huannago.com/search"

# --- 2. ç‹€æ…‹å®šç¾© ---
class AgentState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    knowledge_base: str
    is_hit: bool
    loop_count: int
    target_url: str

# --- 3. æ ¸å¿ƒå·¥å…·å‡½å¼ ---

def search_searxng(query: str):
    """æœå°‹å·¥å…·ï¼šç²å–æœ€ç›¸é—œç¶²å€"""
    params = {"q": query, "format": "json", "language": "zh-TW"}
    try:
        response = requests.get(SEARXNG_URL, params=params, timeout=10)
        results = response.json().get('results', [])
        return results[0].get('url') if results else None
    except:
        return None

def vlm_read_website(url: str) -> str:
    """è¦–è¦ºå·¥å…·ï¼šPlaywright æˆªåœ– + VLM åˆ†æ"""
    print(f"ğŸ“¸ [VLM] å•Ÿå‹•è¦–è¦ºé–±è®€: {url}")
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page(viewport={'width': 1280, 'height': 1200})
            page.goto(url, wait_until="domcontentloaded", timeout=25000)
            page.wait_for_timeout(2000)
            img_b64 = base64.b64encode(page.screenshot()).decode('utf-8')
            browser.close()

        msg = [
            {"type": "text", "text": "è«‹æ ¹æ“šç¶²é æˆªåœ–æ‘˜è¦æ ¸å¿ƒå…§å®¹ï¼ŒåŒ…å«æ•¸æ“šã€æ—¥æœŸèˆ‡äº‹å¯¦ã€‚"},
            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_b64}"}}
        ]
        res = llm.invoke([HumanMessage(content=msg)])
        return re.sub(r'<.*?>', '', res.content).strip()
    except Exception as e:
        return f"è¦–è¦ºåˆ†æå¤±æ•—: {e}"

# --- 4. LangGraph ç¯€é»å¯¦ä½œ ---

def check_cache_node(state: AgentState):
    """å„ªåŒ–æ–¹å¼ï¼šå¿«å–æª¢æŸ¥ (èª¿é¬†åˆ¤æ–·æ¢ä»¶)"""
    query = state["messages"][-1].content.lower()
    # åªè¦åŒ…å« langchain ç›¸é—œå­—çœ¼å°±å‘½ä¸­
    if any(kw in query for kw in ["langchain", "åŸºç¤", "æ¦‚å¿µ"]):
        return {"is_hit": True, "knowledge_base": "å¿«å–å‘½ä¸­ï¼šLangChain æ˜¯å»ºç«‹ LLM æ‡‰ç”¨ç¨‹å¼çš„æ¡†æ¶ï¼Œæ”¯æ´ Chain èˆ‡ Agent çµæ§‹ã€‚", "loop_count": 0}
    return {"is_hit": False, "knowledge_base": "", "loop_count": 0}

def planner_node(state: AgentState):
    """æ±ºç­–ç¯€é»"""
    kb = state.get("knowledge_base", "")
    if not kb: return {"messages": [AIMessage(content="NO")]} # æ²’è³‡æ–™ç›´æ¥èªª NO
    
    prompt = f"å•é¡Œï¼š{state['messages'][0].content}\nè³‡æ–™ï¼š{kb}\nè³‡æ–™æ˜¯å¦è¶³ä»¥å›ç­”ï¼Ÿåªéœ€å› YES æˆ– NOã€‚"
    res = llm.invoke([HumanMessage(content=prompt)])
    clean = re.sub(r'<.*?>', '', res.content).strip().upper()
    return {"messages": [AIMessage(content="YES" if "YES" in clean else "NO")]}

def query_gen_node(state: AgentState):
    """ç”Ÿæˆé—œéµå­—"""
    user_q = state["messages"][0].content
    res = llm.invoke([HumanMessage(content=f"ç‚ºæ­¤å•é¡Œç”¢å‡ºä¸€å€‹æœå°‹é—œéµå­—ï¼š{user_q}")])
    kw = re.sub(r'<.*?>', '', res.content).strip()
    return {"messages": [AIMessage(content=f"é—œéµå­—ï¼š{kw}")], "loop_count": state["loop_count"] + 1}

def search_tool_node(state: AgentState):
    """åŸ·è¡Œæœå°‹"""
    kw = state["messages"][-1].content.replace("é—œéµå­—ï¼š", "")
    url = search_searxng(kw)
    return {"target_url": url}

def vlm_processing_node(state: AgentState):
    """VLM è™•ç†ç¯€é»"""
    url = state.get("target_url")
    if not url: return {"knowledge_base": "æ‰¾ä¸åˆ°ç›¸é—œç¶²é ã€‚"}
    result = vlm_read_website(url)
    return {"knowledge_base": result}

def final_answer_node(state: AgentState):
    """ç”¢å‡ºæœ€çµ‚å›ç­”"""
    kb = state.get("knowledge_base", "")
    res = llm.invoke([HumanMessage(content=f"æ ¹æ“šè³‡æ–™ï¼š{kb}\nå›ç­”å•é¡Œï¼š{state['messages'][0].content}")])
    return {"messages": [AIMessage(content=re.sub(r'<.*?>', '', res.content).strip())]}

# --- 5. æ§‹å»ºåœ–èˆ‡è·¯ç”± ---

workflow = StateGraph(AgentState)
workflow.add_node("check_cache", check_cache_node)
workflow.add_node("planner", planner_node)
workflow.add_node("query_gen", query_gen_node)
workflow.add_node("search_tool", search_tool_node)
workflow.add_node("vlm_processing", vlm_processing_node)
workflow.add_node("final_answer", final_answer_node)

workflow.set_entry_point("check_cache")

# è·¯ç”±åˆ¤æ–·
workflow.add_conditional_edges("check_cache", lambda x: "hit" if x["is_hit"] else "miss", {"hit": "final_answer", "miss": "planner"})

def decision_router(state):
    if state["loop_count"] >= 2: return "y"
    return "y" if "YES" in state["messages"][-1].content.upper() else "n"

workflow.add_conditional_edges("planner", decision_router, {"y": "final_answer", "n": "query_gen"})

workflow.add_edge("query_gen", "search_tool")
workflow.add_edge("search_tool", "vlm_processing")
workflow.add_edge("vlm_processing", "planner")
workflow.add_edge("final_answer", END)

app = workflow.compile()

# --- 6. äº’å‹•ä»‹é¢ ---
if __name__ == "__main__":
    print("\n--- ğŸ¤– å¤§ä½œæ¥­ï¼šè‡ªå‹•æŸ¥è­‰ AI å•Ÿå‹• ---")
    while True:
        user_input = input("\nè«‹è¼¸å…¥å•é¡Œ (q é›¢é–‹): ")
        if user_input.lower() == 'q': break
        
        inputs = {"messages": [HumanMessage(content=user_input)], "knowledge_base": "", "is_hit": False, "loop_count": 0, "target_url": ""}
        
        for event in app.stream(inputs):
            for node, data in event.items():
                print(f"ğŸ“ [ç¯€é»]: {node}")
                if node == "final_answer":
                    print(f"\nğŸ“¢ æœ€çµ‚å›ç­”ï¼š\n{data['messages'][-1].content}")